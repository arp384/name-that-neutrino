{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do_processing.ipynb\n",
    "## Author: Andrew Phillips\n",
    "## Purpose: Selects NTN phase 1 dataset from raw i3 sim files, applies custom modules, and saves to csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os, fnmatch\n",
    "import pandas as pd\n",
    "import glob\n",
    "from icecube import dataio, dataclasses, icetray, MuonGun\n",
    "from I3Tray import *\n",
    "from icecube.hdfwriter import I3HDFWriter\n",
    "import h5py\n",
    "from i3_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in file paths, and corresponding subject set ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_info = pd.read_csv(os.path.join(os.getcwd(), 'ntn_phase1_files.csv'))\n",
    "i3_files = list(file_info['file path'])\n",
    "subj_set_ids = list(file_info['subject_set_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select only desired events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1 of 19\n",
      "112109\n",
      "classifier_DST_IC86.2020_NuMu.021971.000000.i3.bz2\n",
      "Processing file 2 of 19\n",
      "112391\n",
      "classifier_rehyd_DST_IC86.2020_NuMu.021971.000001.i3.zst\n",
      "Processing file 3 of 19\n",
      "112425\n",
      "classifier_rehyd_DST_IC86.2020_NuMu.021971.000002.i3.zst\n",
      "Processing file 4 of 19\n",
      "112433\n",
      "classifier_rehyd_DST_IC86.2020_NuMu.021971.000219.i3.zst\n",
      "Processing file 5 of 19\n",
      "112464\n",
      "classifier_rehyd_DST_IC86.2020_NuMu.021971.000898.i3.zst\n",
      "Processing file 6 of 19\n",
      "112392\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000000.i3.zst\n",
      "Processing file 7 of 19\n",
      "112414\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000001.i3.zst\n",
      "Processing file 8 of 19\n",
      "112418\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000003.i3.zst\n",
      "Processing file 9 of 19\n",
      "112498\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000032.i3.zst\n",
      "Processing file 10 of 19\n",
      "112473\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000052.i3.zst\n",
      "Processing file 11 of 19\n",
      "112487\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000119.i3.zst\n",
      "Processing file 12 of 19\n",
      "112492\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000224.i3.zst\n",
      "Processing file 13 of 19\n",
      "112467\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000348.i3.zst\n",
      "Processing file 14 of 19\n",
      "112116\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000493.i3.zst\n",
      "Processing file 15 of 19\n",
      "112481\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000763.i3.zst\n",
      "Processing file 16 of 19\n",
      "112501\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000873.i3.zst\n",
      "Processing file 17 of 19\n",
      "112118\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.000999.i3.zst\n",
      "Processing file 18 of 19\n",
      "112119\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.001999.i3.zst\n",
      "Processing file 19 of 19\n",
      "112120\n",
      "classifier_rehyd_DST_IC86.2020_NuE.022067.009999.i3.zst\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0, len(subj_set_ids)): #loop over all the ssids\n",
    "    \n",
    "    print(f'Processing file {idx+1} of {len(subj_set_ids)}')\n",
    "    print(subj_set_ids[idx])\n",
    "    print(i3_files[idx].split('/')[-1])\n",
    "    f = os.path.join(os.getcwd(), 'event_ids', f'evt_ids_{subj_set_ids[idx]}.csv') #get path to event ids list\n",
    "    df = pd.read_csv(f)\n",
    "    df = df.sort_values(by=['event_id'])\n",
    "    df = df.drop_duplicates(subset=['event_id'])\n",
    "    event_ids = list(df['event_id']) #TURN DF Into list\n",
    "    event_ids = list(set(event_ids)) #there's some duplicate entries, so get rid of those\n",
    "    subject_ids = list(df['subject_id'])\n",
    "    event_ids.sort() #sort the event_ids. this should speed things up since the \n",
    "    outfile = dataio.I3File(os.path.join('output', f'ntn_events_{subj_set_ids[idx]}.i3'), 'w') #open empty i3 for output\n",
    "    infile = dataio.I3File(i3_files[idx]) #open target i3\n",
    "\n",
    "    while(infile.more()):\n",
    "        frame = infile.pop_daq() #pop frame\n",
    "        evt_head = frame[\"I3EventHeader\"] #get event header\n",
    "        evt_id = evt_head.event_id #get event id\n",
    "        if(evt_id == event_ids[0]): #check if event id is in our list\n",
    "            frame['subject_id'] = icetray.I3Int(subject_ids.pop(0))\n",
    "            outfile.push(frame) #if so, push the frame to our output file\n",
    "            event_ids.pop(0) #remove that value from the list of event ids\n",
    "        if event_ids == []: #stop when we've grabbed all of our event ids\n",
    "            break   \n",
    "                            \n",
    "    outfile.close() #close the files\n",
    "    infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply custom modules, save to csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntn_events_112498.i3\n",
      "ntn_events_112120.i3\n",
      "ntn_events_112433.i3\n",
      "ntn_events_112118.i3\n",
      "ntn_events_112119.i3\n",
      "ntn_events_112425.i3\n",
      "ntn_events_112391.i3\n",
      "ntn_events_112418.i3\n",
      "ntn_events_112392.i3\n",
      "ntn_events_112109.i3\n",
      "ntn_events_112116.i3\n",
      "ntn_events_112501.i3\n",
      "ntn_events_112481.i3\n",
      "ntn_events_112414.i3\n",
      "ntn_events_112464.i3\n",
      "ntn_events_112473.i3\n",
      "ntn_events_112492.i3\n",
      "ntn_events_112467.i3\n",
      "ntn_events_112487.i3\n"
     ]
    }
   ],
   "source": [
    "output_csvs = []\n",
    "for f in [fname for fname in os.listdir(os.path.join(os.getcwd(), 'output')) if fname != 'output']:\n",
    "    print(f)\n",
    "    #(outfile, hd5_name) = apply_modules(os.path.join(os.getcwd(), 'output',f), os.path.join(os.getcwd(), 'output'))\n",
    "    #output_csvs.append(process_data(os.path.join(os.getcwd(), 'output', hd5_name), os.path.join(os.getcwd(), 'output', 'output'), f.split('.')[0].split('_')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate all the csvs into a master df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_csv(output_csvs.pop()) #create a new dataframe\n",
    "\n",
    "for csv in output_csvs: #loop over all the remaining csvs, concatenate to dataframe\n",
    "    \n",
    "    DF = pd.concat([DF, pd.read_csv(csv)])\n",
    "    \n",
    "#DF.drop(['Unnamed: 0'])\n",
    "DF.to_csv('ntn_all_events.csv', index=False) #save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
